---
Fecha de creación: 2025-09-02 00:22
Fecha de Modificación: 2025-09-02 00:22
tags: 
Tema:
---


## 📚 Idea/Concepto 

Un Large Language Model (LLM) es un modelo de inteligencia artificial basado en la arquitectura Transformer, típicamente usando la pila de decodificadores con autoatención enmascarada para generar texto. Representa el lenguaje en tokens (palabras, subpalabras o secuencias de caracteres) y emplea autoatención con codificaciones posicionales para capturar dependencias de corto y largo alcance en paralelo. Cada bloque Transformer incluye redes feed-forward, conexiones residuales y normalización de capa, esenciales para la estabilidad y el aprendizaje profundo. El entrenamiento ocurre en dos etapas: un preentrenamiento masivo con grandes volúmenes de texto y un ajuste fino (fine-tuning) para tareas específicas. Los LLMs poseen miles de millones de parámetros y presentan capacidades emergentes, lo que les permite generar texto coherente y resolver tareas complejas. Sin embargo, su rendimiento está limitado por la ventana de contexto, que restringe la cantidad de información procesable en cada paso.
## 📌 Puntos Claves (Opcional)
- 

## 🔗 Connections
- [[ ]]

## 💡 Personal Insight (Opcional)
- 
## 🧾 Recursos (Opcional)
- 