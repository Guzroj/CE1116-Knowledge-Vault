---
Fecha de creaci贸n: 2025-09-02 00:25
Fecha de Modificaci贸n: 2025-09-02 00:25
tags: 
Tema:
---


##  Idea/Concepto 
Un embedding es una representaci贸n vectorial densa de tokens aprendida durante el entrenamiento, que captura similitudes sem谩nticas y contextuales. En Transformers, los embeddings se combinan con codificaciones posicionales y luego se proyectan en vectores Query, Key y Value (WQ, WK, WV) para alimentar el mecanismo de autoatenci贸n. La similitud entre Q y K se escala por \(\sqrt{d_k}\) antes del softmax, y las salidas ponderadas de cada cabeza se concatenan y proyectan mediante W\_O para formar la representaci贸n contextual final. Tras la atenci贸n, los embeddings pasan por redes feed-forward (FFN), envueltas en conexiones residuales y normalizaci贸n de capa, lo que asegura estabilidad en el entrenamiento y permite construir representaciones sem谩nticas m谩s profundas.

##  Puntos Claves (Opcional)
- 

##  Connections
- [[ ]]

##  Personal Insight (Opcional)
- 
## Ь Recursos (Opcional)
- 