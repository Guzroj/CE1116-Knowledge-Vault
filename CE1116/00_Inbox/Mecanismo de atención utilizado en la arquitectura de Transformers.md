---
Fecha de creaci贸n: 2025-09-02 00:23
Fecha de Modificaci贸n: 2025-09-02 00:23
tags: 
Tema:
---


##  Idea/Concepto 
El mecanismo de atenci贸n en Transformers calcula la relevancia entre tokens para construir representaciones contextuales. Los embeddings se proyectan en Query (Q), Key (K) y Value (V); la similitud entre Q y K se obtiene con un producto punto escalado ($\frac{1}{\sqrt{d_k}}$
), normalizado con softmax, y se usa para ponderar los Values. El resultado se combina con el embedding original mediante conexi贸n residual y normalizaci贸n de capa, asegurando estabilidad. En Multi-Head Attention, cada cabeza tiene proyecciones lineales independientes; sus salidas se concatenan y se proyectan nuevamente para mantener la dimensi贸n del modelo. Este mecanismo se complementa con embeddings posicionales para capturar el orden de los tokens y con redes feed-forward (FFN) que ampl铆an la capacidad representacional. Se aplica como auto-atenci贸n (self-attention) dentro del encoder y como atenci贸n cruzada (cross-attention) en el esquema encoder-decoder, mientras que en el decodificador autorregresivo se usa un enmascaramiento para impedir que un token acceda a posiciones futuras.

##  Puntos Claves (Opcional)
- 

##  Connections
- [[ ]]

##  Personal Insight (Opcional)
- 
## Ь Recursos (Opcional)
- 